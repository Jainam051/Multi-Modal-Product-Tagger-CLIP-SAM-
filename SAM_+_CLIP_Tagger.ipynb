{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jainam051/Multi-Modal-Product-Tagger-CLIP-SAM-/blob/main/SAM_%2B_CLIP_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjJqx1VWBRzP"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install opencv-python matplotlib\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "!pip install supervision\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KJpghOoUjSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import supervision as sv\n",
        "import gradio as gr\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
        "sam.to(device)"
      ],
      "metadata": {
        "id": "8TrWrPlaDJ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHbhVrk0uJ2j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def low_quality(image_path, output_path=\"low_quality.jpg\", quality=10):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "    # Save image with low JPEG quality\n",
        "    cv2.imwrite(output_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXF-cT_5TThg"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBGxMUhAqU1O"
      },
      "outputs": [],
      "source": [
        "def image_preprocess(image_path):\n",
        "  image = cv2.imread(image_path)\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  masks = mask_generator.generate(image_rgb)\n",
        "  return image ,image_rgb , masks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def new_image_preprocess(image):\n",
        "  image_np = np.array(image)\n",
        "  image_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
        "  masks = mask_generator.generate(image_rgb)\n",
        "  return image , image_rgb , masks"
      ],
      "metadata": {
        "id": "jCORXJr5bf2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jYQ_eQsaUbz"
      },
      "outputs": [],
      "source": [
        "def generate_image(image, masks):\n",
        "  mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "  detections = sv.Detections.from_sam(sam_result=masks)\n",
        "  annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "  sv.plot_images_grid(\n",
        "    images=[image, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRqp0gSMgjNa"
      },
      "outputs": [],
      "source": [
        "def generate_result(image):\n",
        "\n",
        "  mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "  image, image_rgb , masks = new_image_preprocess(image)\n",
        "  result = []\n",
        "  label_to_color = {}\n",
        "  #generate_image(image,masks)\n",
        "  with open(\"imagenet_classes.txt\") as f:\n",
        "    class_names = [f\"a photo of a {line.strip()}\" for line in f.readlines()]\n",
        "  text_inputs = clip.tokenize(class_names).to(device)\n",
        "  with torch.no_grad():\n",
        "    text_features = clip_model.encode_text(text_inputs)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  for idx, mask in enumerate(masks):\n",
        "\n",
        "    x0, y0, w, h = mask[\"bbox\"]\n",
        "    x1, y1 = x0 + w, y0 + h\n",
        "\n",
        "    masked_img = image_rgb[y0:y1, x0:x1]\n",
        "\n",
        "\n",
        "    if masked_img.shape[0] < 10 or masked_img.shape[1] < 10:\n",
        "        continue\n",
        "\n",
        "    pil_crop = Image.fromarray(masked_img)\n",
        "    image_input = clip_preprocess(pil_crop).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_input)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (100.0 * image_features @ text_features.T)\n",
        "        probs = similarity.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    pred_idx = probs[0].argmax()\n",
        "    label = class_names[pred_idx]\n",
        "    confidence = probs[0][pred_idx]\n",
        "\n",
        "    if confidence > 0.10 :\n",
        "      if label not in label_to_color:\n",
        "        label_to_color[label] = tuple(random.randint(0, 255) for _ in range(3))\n",
        "      color = label_to_color[label]\n",
        "\n",
        "      label_text = f\"{label} ({confidence:.2f})\"\n",
        "      result.append(((x0,y0,w,h),label_text,color))\n",
        "\n",
        "\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_clip(image):\n",
        "   result=  generate_result(image)\n",
        "   annotated_img = image.convert(\"RGB\")\n",
        "   draw = ImageDraw.Draw(annotated_img)\n",
        "   for item in result:\n",
        "    box = item[0]\n",
        "    title = item[1]\n",
        "    color = item[2]\n",
        "    x0, y0, w, h = box\n",
        "    x1, y1 = x0 + w, y0 + h\n",
        "    draw.rectangle([x0, y0, x1, y1], outline=color, width=3)\n",
        "    bbox = draw.textbbox((0, 0), title)\n",
        "\n",
        "    label_pos = (x1 - bbox[2] - 4, y0 + 4)\n",
        "    label_width = bbox[2] - bbox[0]\n",
        "    label_height = bbox[3] - bbox[1]\n",
        "    draw.rectangle([label_pos, (x1, y0 + label_height + 8)], fill=color)\n",
        "    draw.text(label_pos, title, fill=\"white\")\n",
        "\n",
        "   return annotated_img\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tRzjdduAmjni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4xtPpH5Zb16"
      },
      "outputs": [],
      "source": [
        "# Gradio UI\n",
        "gr.Interface(\n",
        "    fn=segment_clip,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Image(type=\"pil\"),\n",
        "    title=\"Multi-Modal Product Tagger (CLIP + SAM)\"\n",
        ").launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}